{"version":3,"sources":["../../../src/test/genlex/tokenizer_test.js"],"names":["tkBuilder","builder","setUp","done","test","expect","ok","tokenize","ofString","isSuccess","deepEqual","success","keyword","ident","char","string"],"mappings":";;;;;;AAAA;;;;AACA;;;;AACA;;;;;;AAEA,IAAMA,YAAY,gBAAMC,OAAxB;;AAEA;;;;;;;;;;;;;;;;;;;;kBAoBe;AACXC,WAAO,eAASC,IAAT,EAAe;AAClBA;AACH,KAHU;;AAKX,8BAA0B,6BAASC,IAAT,EAAe;AACrCA,aAAKC,MAAL,CAAY,CAAZ;AACA;AACAD,aAAKE,EAAL,CACI,yBAAU,CAAC,GAAD,EAAM,IAAN,CAAV,EACKC,QADL,CACc,gBAAOC,QAAP,CAAgB,iBAAhB,CADd,EAEKC,SAFL,EADJ,EAII,sBAJJ;AAMAL,aAAKD,IAAL;AACH,KAfU;;AAiBX,yCAAqC,sCAASC,IAAT,EAAe;AAChDA,aAAKC,MAAL,CAAY,CAAZ;AACA;AACAD,aAAKM,SAAL,CACI,yBAAU,CAAC,KAAD,EAAQ,IAAR,EAAc,GAAd,EAAmB,IAAnB,CAAV,EACKH,QADL,CACc,gBAAOC,QAAP,CAAgB,uBAAhB,CADd,EAEKG,OAFL,EADJ,EAII,CACIX,UAAUY,OAAV,CAAkB,KAAlB,CADJ,EAEIZ,UAAUa,KAAV,CAAgB,GAAhB,CAFJ,EAGIb,UAAUY,OAAV,CAAkB,GAAlB,CAHJ,EAIIZ,UAAUc,IAAV,CAAe,GAAf,CAJJ,EAKId,UAAUY,OAAV,CAAkB,IAAlB,CALJ,EAMIZ,UAAUe,MAAV,CAAiB,IAAjB,CANJ,CAJJ,EAYI,+BAZJ;AAcAX,aAAKD,IAAL;AACH;AAnCU,C","file":"tokenizer_test.js","sourcesContent":["import stream from '../../lib/stream/index';\nimport tokenizer from '../../lib/genlex/tokenizer';\nimport token from '../../lib/genlex/token';\n\nconst tkBuilder = token.builder;\n\n/*\n  ======== A Handy Little Nodeunit Reference ========\n  https://github.com/caolan/nodeunit\n\n  Test methods:\n    test.expect(numAssertions)\n    test.done()\n  Test assertions:\n    test.ok(value, [message])\n    test.equal(actual, expected, [message])\n    test.notEqual(actual, expected, [message])\n    test.deepEqual(actual, expected, [message])\n    test.notDeepEqual(actual, expected, [message])\n    test.strictEqual(actual, expected, [message])\n    test.notStrictEqual(actual, expected, [message])\n    test.throws(block, [error], [message])\n    test.doesNotThrow(block, [error], [message])\n    test.ifError(value)\n*/\n\nexport default {\n    setUp: function(done) {\n        done();\n    },\n\n    'tokeniser is a success': function(test) {\n        test.expect(1);\n        // tests here\n        test.ok(\n            tokenizer([':', '->'])\n                .tokenize(stream.ofString('type f : a -> b'))\n                .isSuccess(),\n            'should be a success.'\n        );\n        test.done();\n    },\n\n    'tokeniser return a list of tokens': function(test) {\n        test.expect(1);\n        // tests here\n        test.deepEqual(\n            tokenizer(['let', 'in', '=', '->'])\n                .tokenize(stream.ofString('let f = \\'a\\' in \"aa\"'))\n                .success(),\n            [\n                tkBuilder.keyword('let'),\n                tkBuilder.ident('f'),\n                tkBuilder.keyword('='),\n                tkBuilder.char('a'),\n                tkBuilder.keyword('in'),\n                tkBuilder.string('aa'),\n            ],\n            'should be a a list of tokens.'\n        );\n        test.done();\n    },\n};\n"]}