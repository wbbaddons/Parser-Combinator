{"version":3,"sources":["../../../src/test/genlex/tokenizer_test.js"],"names":["tkBuilder","builder","setUp","done","test","expect","ok","tokenize","ofString","isSuccess","deepEqual","success","keyword","ident","char","string"],"mappings":";;;;;;AAAA;;;;AACA;;;;AACA;;;;;;AAEA,IAAMA,YAAY,gBAAMC,OAAxB;;AAEA;;;;;;;;;;;;;;;;;;;;kBAoBe;AACXC,WAAO,eAASC,IAAT,EAAe;AAClBA;AACH,KAHU;;AAKX,8BAA0B,6BAASC,IAAT,EAAe;AACrCA,aAAKC,MAAL,CAAY,CAAZ;AACA;AACAD,aAAKE,EAAL,CACI,yBAAU,CAAC,GAAD,EAAM,IAAN,CAAV,EACKC,QADL,CACc,gBAAOC,QAAP,CAAgB,iBAAhB,CADd,EAEKC,SAFL,EADJ,EAII,sBAJJ;AAMAL,aAAKD,IAAL;AACH,KAfU;;AAiBX,yCAAqC,sCAASC,IAAT,EAAe;AAChDA,aAAKC,MAAL,CAAY,CAAZ;AACA;AACAD,aAAKM,SAAL,CACI,yBAAU,CAAC,KAAD,EAAQ,IAAR,EAAc,GAAd,EAAmB,IAAnB,CAAV,EACKH,QADL,CACc,gBAAOC,QAAP,CAAgB,uBAAhB,CADd,EAEKG,OAFL,EADJ,EAII,CACIX,UAAUY,OAAV,CAAkB,KAAlB,CADJ,EAEIZ,UAAUa,KAAV,CAAgB,GAAhB,CAFJ,EAGIb,UAAUY,OAAV,CAAkB,GAAlB,CAHJ,EAIIZ,UAAUc,IAAV,CAAe,GAAf,CAJJ,EAKId,UAAUY,OAAV,CAAkB,IAAlB,CALJ,EAMIZ,UAAUe,MAAV,CAAiB,IAAjB,CANJ,CAJJ,EAYI,+BAZJ;AAcAX,aAAKD,IAAL;AACH;AAnCU,C","file":"tokenizer_test.js","sourcesContent":["import stream from '../../lib/stream/index';\r\nimport tokenizer from '../../lib/genlex/tokenizer';\r\nimport token from '../../lib/genlex/token';\r\n\r\nconst tkBuilder = token.builder;\r\n\r\n/*\r\n  ======== A Handy Little Nodeunit Reference ========\r\n  https://github.com/caolan/nodeunit\r\n\r\n  Test methods:\r\n    test.expect(numAssertions)\r\n    test.done()\r\n  Test assertions:\r\n    test.ok(value, [message])\r\n    test.equal(actual, expected, [message])\r\n    test.notEqual(actual, expected, [message])\r\n    test.deepEqual(actual, expected, [message])\r\n    test.notDeepEqual(actual, expected, [message])\r\n    test.strictEqual(actual, expected, [message])\r\n    test.notStrictEqual(actual, expected, [message])\r\n    test.throws(block, [error], [message])\r\n    test.doesNotThrow(block, [error], [message])\r\n    test.ifError(value)\r\n*/\r\n\r\nexport default {\r\n    setUp: function(done) {\r\n        done();\r\n    },\r\n\r\n    'tokeniser is a success': function(test) {\r\n        test.expect(1);\r\n        // tests here\r\n        test.ok(\r\n            tokenizer([':', '->'])\r\n                .tokenize(stream.ofString('type f : a -> b'))\r\n                .isSuccess(),\r\n            'should be a success.'\r\n        );\r\n        test.done();\r\n    },\r\n\r\n    'tokeniser return a list of tokens': function(test) {\r\n        test.expect(1);\r\n        // tests here\r\n        test.deepEqual(\r\n            tokenizer(['let', 'in', '=', '->'])\r\n                .tokenize(stream.ofString('let f = \\'a\\' in \"aa\"'))\r\n                .success(),\r\n            [\r\n                tkBuilder.keyword('let'),\r\n                tkBuilder.ident('f'),\r\n                tkBuilder.keyword('='),\r\n                tkBuilder.char('a'),\r\n                tkBuilder.keyword('in'),\r\n                tkBuilder.string('aa'),\r\n            ],\r\n            'should be a a list of tokens.'\r\n        );\r\n        test.done();\r\n    },\r\n};\r\n"]}